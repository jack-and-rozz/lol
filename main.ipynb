{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from main import ExperimentManager, get_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup args.\n",
    "parser = get_parser()\n",
    "ROOT = '.'\n",
    "#args = [ROOT + '/checkpoints/tmp', 'demo', '-ct=tmp', '--cleanup=T']\n",
    "args = [ROOT + '/checkpoints/demo', 'demo',]\n",
    "args = parser.parse_args(args=args)\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found an existing config file, './checkpoints/demo/main.conf'.\n",
      "{'trainer': {'optimizer': {'optimizer_type': 'AdamOptimizer', 'max_gradient_norm': 1.0, 'learning_rate': 0.0001, 'decay_rate_per_epoch': 0.9}, 'max_to_keep': 1, 'max_epoch': 60, 'dropout_rate': 0.2, 'lexical_dropout_rate': 0.0, 'trainer_type': 'MultiGPUTrainer'}, 'tasks': {'dialogue_en': {'model_type': 'CrossLingualResponseGeneration', 'loss_weight': 1.0, 'batch_size': 500, 'num_ffnn_layers': {'task': 2, 'lang': 1}, 'encoder': {'cell_type': 'GRUCell', 'rnn_size': 500, 'num_layers': 3, 'use_pretrained_emb': True, 'use_residual': True, 'use_birnn': True, 'embedding_size': {'char': 8}}, 'decoder': {'cell_type': 'GRUCell', 'rnn_size': 500, 'num_layers': 3, 'use_residual': True, 'maxlen': 15, 'beam_width': 10, 'length_penalty_weight': 0.6, 'decoder_type': 'AttentionDecoder', 'attention_type': 'LuongAttention', 'use_attention_input_feeding': True, 'top_attention': True, 'use_byway_attention': True}, 'dataset': {'filename': {'train': 'train.txt', 'valid': 'dev.txt', 'test': 'test.txt'}, 'iterations_per_epoch': 0, 'minlen': {'word': 0, 'char': 5}, 'maxlen': {'word': 20, 'char': 10}, 'max_rows': {'train': 0, 'valid': 0, 'test': 0}, 'dataset_type': 'PairwiseDialogueDataset', 'lang': {'input': 'en', 'output': 'en'}, 'source_dir': 'dataset/baheti'}}}, 'vocab': {'encoder': {'word': {'vocab_size': 15000, 'trainable': True, 'lowercase': True, 'normalize_digits': True, 'centralize_embedding': True, 'normalize_embedding': True, 'split_quotation': False, 'use_nltk_tokenizer': False, 'pad_token': '</s>', 'unk_token': '<unk>', 'bos_token': '<bos>', 'emb_config': {'path': 'embeddings/word2vec/opensubtitles.en.vec', 'size': 300, 'skip_first': True}}, 'char': {'vocab_size': 5000, 'split_quotation': False, 'use_nltk_tokenizer': False, 'pad_token': '</s>', 'unk_token': '<unk>', 'vocab_path': 'embeddings/char_vocab.en.txt'}}, 'decoder': {'word': {'vocab_size': 15000, 'trainable': True, 'lowercase': True, 'normalize_digits': True, 'centralize_embedding': True, 'normalize_embedding': True, 'split_quotation': False, 'use_nltk_tokenizer': False, 'pad_token': '</s>', 'unk_token': '<unk>', 'bos_token': '<bos>', 'emb_config': {'path': 'embeddings/word2vec/opensubtitles.en.vec', 'size': 300, 'skip_first': True}}, 'char': {'vocab_size': 5000, 'split_quotation': False, 'use_nltk_tokenizer': False, 'pad_token': '</s>', 'unk_token': '<unk>', 'vocab_path': 'embeddings/char_vocab.en.txt'}}}}\n",
      "Loading word embeddings from embeddings/word2vec/opensubtitles.en.vec...\n",
      "Done loading word embeddings.\n",
      "[INFO] 2019-01-31 17:13:06 - init_vocab: 2.419578 sec\n",
      "Done loading the vocabulary.\n",
      "Loading word embeddings from embeddings/word2vec/opensubtitles.en.vec...\n",
      "Done loading word embeddings.\n",
      "[INFO] 2019-01-31 17:13:08 - init_vocab: 2.267448 sec\n",
      "[INFO] 2019-01-31 17:13:08 - __init__: 4.753441 sec\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "    manager = ExperimentManager(args, sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-01-31 17:16:33 - create_model: 0.000024 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Context   : how are you ?\n",
      "- Hypothesis00: what are you doing here ?\n",
      "- Hypothesis01: i 'm glad you 'r e here\n",
      "- Hypothesis02: i 'm glad to see you\n",
      "- Hypothesis03: i <unk> l be right back\n",
      "- Hypothesis04: i 'm sorry i 'm late\n",
      "- Hypothesis05: i <unk> e been looking for you\n",
      "- Hypothesis06: i 'm glad to see you again\n",
      "- Hypothesis07: i 'm glad to be here\n",
      "- Hypothesis08: i 'm glad to hear that\n",
      "- Hypothesis09: i 'm glad to see you 'r e here\n",
      "\n",
      "- Context   : do you have a pet ?\n",
      "- Hypothesis00: do you have a pet ?\n",
      "- Hypothesis01: do you have a pet dog ?\n",
      "- Hypothesis02: do you have a dog ?\n",
      "- Hypothesis03: do you have a pet cat ?\n",
      "- Hypothesis04: do you have a <unk> ?\n",
      "- Hypothesis05: do you have a pet animal ?\n",
      "- Hypothesis06: do you have a pet <unk> ?\n",
      "- Hypothesis07: i don 't have a pet\n",
      "- Hypothesis08: do you have a pet name ?\n",
      "- Hypothesis09: i don 't have a pet dog\n",
      "\n",
      "- Context   : look at this \u001b[4m.\u001b[0m\n",
      "- Hypothesis00: i <unk> e never seen anything like it\n",
      "- Hypothesis01: i <unk> e never seen anything like this\n",
      "- Hypothesis02: i <unk> e never seen anything like this before\n",
      "- Hypothesis03: i <unk> e never seen this before\n",
      "- Hypothesis04: it 's a <unk> <unk> <unk>\n",
      "- Hypothesis05: i <unk> e never seen anything like it before\n",
      "- Hypothesis06: i <unk> e never seen it before\n",
      "- Hypothesis07: i <unk> e never seen anything like that\n",
      "- Hypothesis08: i <unk> e never seen anything like that before\n",
      "- Hypothesis09: i <unk> e never seen anything like this in my life\n",
      "\n",
      "- Context   : how old are you ?\n",
      "- Hypothesis00: how old are you now ?\n",
      "- Hypothesis01: how old are you ? !\n",
      "- Hypothesis02: i don 't know what you 'r e talking about\n",
      "- Hypothesis03: i don 't know what to say\n",
      "- Hypothesis04: i don 't know how old i am\n",
      "- Hypothesis05: i don 't know how old you are\n",
      "- Hypothesis06: i don 't know what to do\n",
      "- Hypothesis07: i don 't know what i 'm talking about\n",
      "- Hypothesis08: i don 't know what i 'm doing\n",
      "- Hypothesis09: i don 't know what i 'm going to do\n",
      "\n",
      "- Context   : can i have something to drink ?\n",
      "- Hypothesis00: i <unk> l be right back\n",
      "- Hypothesis01: i <unk> l be right there\n",
      "- Hypothesis02: i <unk> l see you later\n",
      "- Hypothesis03: i <unk> e got something to drink\n",
      "- Hypothesis04: i <unk> l have a beer\n",
      "- Hypothesis05: i <unk> l give you something to drink\n",
      "- Hypothesis06: i <unk> l have a cup of coffee\n",
      "- Hypothesis07: i <unk> l be back in a minute\n",
      "- Hypothesis08: i <unk> l give you a call\n",
      "- Hypothesis09: i <unk> l have a cup of tea\n",
      "\n",
      "- Context   : i am chatting with you\n",
      "- Hypothesis00: i don 't know what you 'r e talking about\n",
      "- Hypothesis01: i don 't know what to do\n",
      "- Hypothesis02: i don 't want to talk about it\n",
      "- Hypothesis03: i have to go to the bathroom\n",
      "- Hypothesis04: i don 't want to talk to you\n",
      "- Hypothesis05: i don 't know what to say\n",
      "- Hypothesis06: i don 't want to be rude\n",
      "- Hypothesis07: i don 't want to be disturbed\n",
      "- Hypothesis08: i don 't want to talk about this\n",
      "- Hypothesis09: i don 't know what you 'r e talking about but i 'm not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anything = ['how are you ?', 'do you have a pet ?', 'look at this .', 'how old are you ?', 'can i have something to drink ?', 'i am chatting with you']  \n",
    "cornell_dev = ['where does he work ?', \n",
    "                          'here ’s your jacket !', \n",
    "                          'what ’s so damn funny ?', \n",
    "                          'well , what exactly does our platoon do ? serve <unk>? process paperwork ?']\n",
    "opensubtitles_train = [\n",
    "    'why a second weapon ?',\n",
    "    'where were you going to take it ?',\n",
    "    'have everything brought to cargo bay two',\n",
    "    \"you scare off the customers\",\n",
    "    \"hey hey hey hey i didn 't do anything !\",\n",
    "]\n",
    "utterances = anything\n",
    "manager.demo(utterances)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
