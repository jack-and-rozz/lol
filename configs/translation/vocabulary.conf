###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"

word2vec_en_300d {
  path = ${embeddings_dir}/word2vec/aspec.en.vec
  size = 300
  skip_first = true
}

word2vec_ja_300d {
  path = ${embeddings_dir}/word2vec/aspec.ja.vec
  size = 300
  skip_first = true
}

#################################
#    Vocabulary
#################################
pad_token = </s>
unk_token = <unk>
bos_token = <bos>
vocab_base {
  encoder = {
    word = {
      vocab_size = 30000    # Number of tokens loaded per file.
      trainable = true
      lowercase = true
      normalize_digits = true
      centralize_embedding = true
      normalize_embedding = true
      split_quotation = false #true
      use_nltk_tokenizer = false

      pad_token = ${pad_token}
      unk_token = ${unk_token}
      bos_token = ${bos_token}
    }
    char = {
      vocab_size = 5000    # Number of tokens loaded per file.
      split_quotation = ${vocab_base.encoder.word.split_quotation}
      use_nltk_tokenizer = ${vocab_base.encoder.word.use_nltk_tokenizer}

      pad_token = ${pad_token}
      unk_token = ${unk_token}
    }
  }
}


vocab_en_ja = ${vocab_base}{
  encoder = {
    word = {
      emb_config = ${word2vec_en_300d} #${fasttext_en_300d}
    }
    char = {
      vocab_path = ${embeddings_dir}/char_vocab.en.txt
    }
  }
  decoder = {
    word = ${vocab_base.encoder.word}{
      emb_config = ${word2vec_ja_300d}
    }
  }
}
