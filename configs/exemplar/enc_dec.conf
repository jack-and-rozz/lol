#################################
#    Encoder / Decoder
#################################

rnn_size = 500
cell_type = GRUCell
use_residual = true
encoder = {
  word = {
    use_pretrained_emb = true
    embedding_size = {
      #char = 8          # If 0, characters are not utilized.
      char = 0
    }
  }
  sent = {
    rnn_size = ${rnn_size}
    cell_type = ${cell_type}
    num_layers = 2
    use_birnn = true    # If true, an another bi-directional RNN layer is attached to the bottom of encoder (i.e. the number of layers becomes num_layers + 1).
    use_residual = ${use_residual}
  }
  dial = {
    rnn_size = ${rnn_size}
    cell_type = ${cell_type}
    num_layers = 2
    use_birnn = false
    use_residual = ${use_residual}
  }
}

decoder = {
  cell_type = ${cell_type}
  rnn_size = ${rnn_size}
  num_layers = 2
  use_residual = ${use_residual}
  maxlen = 15
  beam_width = 10
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}


rnn_decoder = ${decoder}{
  decoder_type = RNNDecoder
}
attn_decoder = ${decoder}{
  decoder_type = AttentionDecoder
  attention_type = LuongAttention
  use_attention_input_feeding = true
  top_attention = true       # If true, only the top of stacked-RNN apply attention-mechanism to encoder's outputs.
  use_byway_attention = true # Whether prepend a state representing "no attention" to attention_states.
}
