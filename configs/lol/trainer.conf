###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  max_gradient_norm = 1.0
  learning_rate = 0.0001 # 1e-4
  #learning_rate = 0.00001 # 1e-5
  decay_rate_per_epoch = 0.75

  # decay_rate = 0.999
  # decay_frequency = 10000 # 0.999 ** (100000*300/10000) = 0.05
}
sgd {
  optimizer_type = GradientDescentOptimizer
  max_gradient_norm = 5.0
  learning_rate = 0.1
  decay_rate_per_epoch = 0.95
  # decay_rate = 0.999
  # decay_frequency = 1000
}


###################################
#         Trainer
###################################

trainer_base {
  #trainer_type = 
  optimizer = ${adam}
  max_to_keep = 1
  max_epoch = 30
  dropout_rate = 0.2   # keep_prob = 1.0 - dropout_rate
  lexical_dropout_rate = 0.0
}

gradient_sum = ${trainer_base}{
  trainer_type = GradientSum
}

multi_gpu = ${trainer_base}{
  trainer_type = MultiGPUTrainer
}
