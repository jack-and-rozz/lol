#################################
#    Encoder / Decoder
#################################

encoder = {
  cell_type = GRUCell
  #cell = LSTMCell
  rnn_size = 500
  num_layers = 3
  use_pretrained_emb = true
  use_residual = true
  use_birnn = true    # If true, an another bi-directional RNN layer is attached to the bottom of encoder (i.e. the number of layers becomes num_layers + 1).

  embedding_size = {
    char = 8
  }
}

decoder = {
  cell_type = ${encoder.cell_type}
  rnn_size = ${encoder.rnn_size}
  num_layers = ${encoder.num_layers}
  use_residual = ${encoder.use_residual}

  maxlen = 15
  beam_width = 10
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}


rnn_decoder = ${decoder}{
  decoder_type = RNNDecoder
}
attn_decoder = ${decoder}{
  decoder_type = AttentionDecoder
  attention_type = LuongAttention
  use_attention_input_feeding = true
  top_attention = true       # If true, only the top of stacked-RNN apply attention-mechanism to encoder's outputs.
  use_byway_attention = true # Whether prepend a state representing "no attention" to attention_states.
}
