###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"
fasttext_en_300d {
  path = ${embeddings_dir}/fasttext/wiki.en.vec
  size = 300
  skip_first = true
}
fasttext_ja_300d {
  path = ${embeddings_dir}/fasttext/wiki.ja.vec
  size = 300
  skip_first = true
}
fasttext_en_300d_mapped {
  path = ${embeddings_dir}/fasttext/wiki.en.vec.mapped
  size = 300
  skip_first = true
}
fasttext_ja_300d_mapped {
  path = ${embeddings_dir}/fasttext/wiki.ja.vec.mapped
  size = 300
  skip_first = true
}

word2vec_en_300d {
  path = ${embeddings_dir}/word2vec/opensubtitles.en.vec
  size = 300
  skip_first = true
}

word2vec_ja_300d {
  path = ${embeddings_dir}/word2vec/opensubtitles.ja.vec
  size = 300
  skip_first = true
}
word2vec_en_300d_cross {
  path = ${embeddings_dir}/word2vec/opensubtitles.en.vec.wordnet_map
  size = 300
  skip_first = true
}
word2vec_ja_300d_cross {
  path = ${embeddings_dir}/word2vec/opensubtitles.ja.vec.wordnet_map
  size = 300
  skip_first = true
}

glove_300d {
  path = ${embeddings_dir}/glove.840B.300d.txt
  size = 300
  skip_first = false
}
turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = false
}


#################################
#    Vocabulary
#################################
pad_token = </s>
unk_token = <unk>
bos_token = <bos>
vocab {
  encoder = {
    word = {
      vocab_size = 15000    # Number of tokens loaded per file.
      trainable = true
      lowercase = true
      normalize_digits = true
      centralize_embedding = true
      normalize_embedding = true
      split_quotation = false #true
      use_nltk_tokenizer = false

      pad_token = ${pad_token}
      unk_token = ${unk_token}
      bos_token = ${bos_token}
    }
    char = {
      vocab_size = 5000    # Number of tokens loaded per file.
      split_quotation = ${vocab.encoder.word.split_quotation}
      use_nltk_tokenizer = ${vocab.encoder.word.use_nltk_tokenizer}

      pad_token = ${pad_token}
      unk_token = ${unk_token}
    }
  }
}


vocab_en = ${vocab}{
  encoder = {
    word = {
      emb_config = ${word2vec_en_300d} #${fasttext_en_300d}
    }
    char = {
      vocab_path = ${embeddings_dir}/char_vocab.en.txt
    }
  }
  decoder = ${vocab_en.encoder}
}
vocab_ja = ${vocab}{
  encoder = {
    word = {
      emb_config = [${word2vec_ja_300d}] #${fasttext_ja_300d}
    }
    char = {
      vocab_path = [${embeddings_dir}/char_vocab.ja.txt]
    }
  }
  decoder = ${vocab_ja.encoder}
}
vocab_en_ja_cross = ${vocab}{
  encoder = {
    word = {
      emb_config = [
        ${word2vec_en_300d_cross}, 
        ${word2vec_ja_300d_cross}
      ] #${fasttext_en_300d}
      trainable = false
    }
    char = {
      vocab_path = [
        ${embeddings_dir}/char_vocab.en.txt, 
        ${embeddings_dir}/char_vocab.ja.txt
      ]
    }
  }
  decoder = ${vocab_en_ja_cross.encoder}
}